{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from tqdm import trange\n",
    "from torch import nn\n",
    "from DDPG_agent import RandomAgent\n",
    "from DDPG import Actor, Critic\n",
    "from DDPG_soft_updates import soft_updates\n",
    "\n",
    "def running_average(x, N):\n",
    "    ''' Function used to compute the running average\n",
    "        of the last N elements of a vector x\n",
    "    '''\n",
    "    if len(x) >= N:\n",
    "        y = np.copy(x)\n",
    "        y[N-1:] = np.convolve(x, np.ones((N, )) / N, mode='valid')\n",
    "    else:\n",
    "        y = np.zeros_like(x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:   0%|          | 0/300 [00:00<?, ?it/s]/tmp/ipykernel_169337/3031554204.py:54: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  states, actions, rewards, next_states, dones = map(lambda x: torch.tensor(x).float().reshape((batch_size, -1)), zip(*sample_experience_replay_buffer(batch_size)))\n",
      "Episode 11 - Reward/Steps: -414.9/1429 - Avg. Reward/Steps: 0.0/0:   4%|â–         | 12/300 [12:11<9:17:00, 116.04s/it]"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env.reset()\n",
    "\n",
    "# State/action space description\n",
    "S = 8\n",
    "A = 2\n",
    "E = S + A + 1 + S + 1 # S, A, R, S', done\n",
    "\n",
    "# Utility parameters\n",
    "n_ep_running_average = 50                    \n",
    "m = len(env.action_space.high) # dimensionality of the action\n",
    "dim_state = len(env.observation_space.high)  \n",
    "\n",
    "# Hyperparameters\n",
    "ERB_size = 30000\n",
    "epsilon0 = 0.99\n",
    "batch_size = 64\n",
    "N_episodes = 300\n",
    "discount_factor = 0.99\n",
    "learning_rate = 0.0005\n",
    "tau = 0.001\n",
    "d = 2\n",
    "mu = 0.15\n",
    "sigma = 0.2\n",
    "hidden_layer_size = 64\n",
    "\n",
    "for discount_factor in [ 0.2, 0.99, 1 ]:\n",
    "    episode_reward_list = [] \n",
    "    episode_number_of_steps = [] \n",
    "    # Experience replay buffer\n",
    "    experience_replay_buffer = deque(maxlen=ERB_size)\n",
    "    def sample_experience_replay_buffer(batch_size):\n",
    "        indices = np.random.choice(len(experience_replay_buffer), batch_size, replace=False)\n",
    "        return [ experience_replay_buffer[index] for index in indices ]\n",
    "    \n",
    "    # Initialize networks, optimizer and loss function\n",
    "    \n",
    "    target_actor = Actor(S, A)\n",
    "    target_critic = Critic(S, A)\n",
    "    network_actor = Actor(S, A)\n",
    "    network_critic = Critic(S, A)\n",
    "    \n",
    "    optimizer_critic = torch.optim.Adam(network_critic.parameters(), lr=learning_rate)\n",
    "    optimizer_actor = torch.optim.Adam(network_actor.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # Functions used to update the networks\n",
    "    def train_target(network_actor: Actor, network_critic: Critic, target_actor: Actor, target_critic: Critic):\n",
    "        soft_updates(network_actor, target_actor, tau)\n",
    "        soft_updates(network_critic, target_critic, tau)\n",
    "    \n",
    "    def train_network_critic(network_actor: Actor, network_critic: Critic, target_actor: Actor, target_critic: Critic):\n",
    "        # create torch tensors from experience replay buffer\n",
    "        states, actions, rewards, next_states, dones = map(lambda x: torch.tensor(x).float().reshape((batch_size, -1)), zip(*sample_experience_replay_buffer(batch_size)))\n",
    "    \n",
    "        # compute loss\n",
    "        y = rewards + discount_factor * target_critic(next_states, target_actor(states)) * (1 - dones)\n",
    "        q = network_critic(states, actions)\n",
    "        loss = loss_fn(y, q)\n",
    "    \n",
    "        # train\n",
    "        optimizer_critic.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(network_critic.parameters(), max_norm=1)\n",
    "        optimizer_critic.step()\n",
    "    \n",
    "    def train_network_actor(network_actor: Actor, network_critic: Critic):\n",
    "        # create torch tensors from experience replay buffer\n",
    "        states, actions, rewards, next_states, dones = map(lambda x: torch.tensor(x).float(), zip(*sample_experience_replay_buffer(batch_size)))\n",
    "    \n",
    "        # compute loss\n",
    "        loss = -torch.mean(network_critic(states, network_actor(states)))\n",
    "    \n",
    "        # train\n",
    "        optimizer_actor.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(network_actor.parameters(), max_norm=1)\n",
    "        optimizer_actor.step()\n",
    "    \n",
    "    while True:\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        for _ in range(1600):\n",
    "            if done: break\n",
    "            action = RandomAgent(m).forward(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "            experience_replay_buffer.append((state, action, reward, next_state, done))\n",
    "            \n",
    "            state = next_state\n",
    "    \n",
    "        if len(experience_replay_buffer) == ERB_size:\n",
    "            break\n",
    "        \n",
    "    noise = np.zeros((1, 2,))\n",
    "    EPISODES = trange(N_episodes, desc='Episode: ', leave=True)\n",
    "    for i in EPISODES:\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        total_episode_reward = 0.\n",
    "        epsilon = epsilon0 * np.exp(- 3 * i / N_episodes)\n",
    "        t = 0\n",
    "        #while not done:\n",
    "            # act\n",
    "        for _ in range(1600):\n",
    "            if done: break\n",
    "            action = network_actor.act(state, noise).ravel()\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            total_episode_reward += reward\n",
    "    \n",
    "            # train\n",
    "            train_network_critic(network_actor, network_critic, target_actor, target_critic)\n",
    "            if t % d == 0:\n",
    "                train_network_actor(network_actor, network_critic)\n",
    "                train_target(network_actor, network_critic, target_actor, target_critic)\n",
    "            \n",
    "            # update\n",
    "            experience_replay_buffer.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            t += 1\n",
    "            noise = -mu * noise + sigma * np.random.randn(1, 2)\n",
    "    \n",
    "        episode_reward_list.append(total_episode_reward)\n",
    "        episode_number_of_steps.append(t)\n",
    "    \n",
    "        env.close()\n",
    "    \n",
    "        EPISODES.set_description(\n",
    "            \"Episode {} - Reward/Steps: {:.1f}/{} - Avg. Reward/Steps: {:.1f}/{}\".format(\n",
    "            i, total_episode_reward, t,\n",
    "            running_average(episode_reward_list, n_ep_running_average)[-1],\n",
    "            running_average(episode_number_of_steps, n_ep_running_average)[-1]))\n",
    "        #if running_average(episode_reward_list, n_ep_running_average)[-1] > 200:\n",
    "        #    break\n",
    "    \n",
    "    while len(episode_reward_list) < N_episodes:\n",
    "        episode_reward_list.append(episode_reward_list[-1])\n",
    "        episode_number_of_steps.append(episode_number_of_steps[-1])\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 9))\n",
    "    ax[0].plot([i for i in range(1, N_episodes+1)], episode_reward_list, label='Episode reward')\n",
    "    ax[0].plot([i for i in range(1, N_episodes+1)], running_average(\n",
    "        episode_reward_list, n_ep_running_average), label='Avg. episode reward')\n",
    "    ax[0].set_xlabel('Episodes')\n",
    "    ax[0].set_ylabel('Total reward')\n",
    "    ax[0].set_title('Total Reward vs Episodes')\n",
    "    ax[0].legend()\n",
    "    ax[0].grid(alpha=0.3)\n",
    "    \n",
    "    ax[1].plot([i for i in range(1, N_episodes+1)], episode_number_of_steps, label='Steps per episode')\n",
    "    ax[1].plot([i for i in range(1, N_episodes+1)], running_average(\n",
    "        episode_number_of_steps, n_ep_running_average), label='Avg. number of steps per episode')\n",
    "    ax[1].set_xlabel('Episodes')\n",
    "    ax[1].set_ylabel('Total number of steps')\n",
    "    ax[1].set_title('Total number of steps vs Episodes')\n",
    "    ax[1].legend()\n",
    "    ax[1].grid(alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env.reset()\n",
    "\n",
    "# State/action space description\n",
    "S = 8\n",
    "A = 2\n",
    "E = S + A + 1 + S + 1 # S, A, R, S', done\n",
    "\n",
    "# Utility parameters\n",
    "n_ep_running_average = 50                    \n",
    "m = len(env.action_space.high) # dimensionality of the action\n",
    "dim_state = len(env.observation_space.high)  \n",
    "\n",
    "# Hyperparameters\n",
    "ERB_size = 30000\n",
    "epsilon0 = 0.99\n",
    "batch_size = 64\n",
    "N_episodes = 300\n",
    "discount_factor = 0.99\n",
    "learning_rate = 0.0005\n",
    "tau = 0.001\n",
    "d = 2\n",
    "mu = 0.15\n",
    "sigma = 0.2\n",
    "hidden_layer_size = 64\n",
    "\n",
    "for ERB_size in [ 10000, 30000, 50000 ]:\n",
    "    episode_reward_list = [] \n",
    "    episode_number_of_steps = [] \n",
    "    # Experience replay buffer\n",
    "    experience_replay_buffer = deque(maxlen=ERB_size)\n",
    "    def sample_experience_replay_buffer(batch_size):\n",
    "        indices = np.random.choice(len(experience_replay_buffer), batch_size, replace=False)\n",
    "        return [ experience_replay_buffer[index] for index in indices ]\n",
    "    \n",
    "    # Initialize networks, optimizer and loss function\n",
    "    \n",
    "    target_actor = Actor(S, A)\n",
    "    target_critic = Critic(S, A)\n",
    "    network_actor = Actor(S, A)\n",
    "    network_critic = Critic(S, A)\n",
    "    \n",
    "    optimizer_critic = torch.optim.Adam(network_critic.parameters(), lr=learning_rate)\n",
    "    optimizer_actor = torch.optim.Adam(network_actor.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # Functions used to update the networks\n",
    "    def train_target(network_actor: Actor, network_critic: Critic, target_actor: Actor, target_critic: Critic):\n",
    "        soft_updates(network_actor, target_actor, tau)\n",
    "        soft_updates(network_critic, target_critic, tau)\n",
    "    \n",
    "    def train_network_critic(network_actor: Actor, network_critic: Critic, target_actor: Actor, target_critic: Critic):\n",
    "        # create torch tensors from experience replay buffer\n",
    "        states, actions, rewards, next_states, dones = map(lambda x: torch.tensor(x).float().reshape((batch_size, -1)), zip(*sample_experience_replay_buffer(batch_size)))\n",
    "    \n",
    "        # compute loss\n",
    "        y = rewards + discount_factor * target_critic(next_states, target_actor(states)) * (1 - dones)\n",
    "        q = network_critic(states, actions)\n",
    "        loss = loss_fn(y, q)\n",
    "    \n",
    "        # train\n",
    "        optimizer_critic.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(network_critic.parameters(), max_norm=1)\n",
    "        optimizer_critic.step()\n",
    "    \n",
    "    def train_network_actor(network_actor: Actor, network_critic: Critic):\n",
    "        # create torch tensors from experience replay buffer\n",
    "        states, actions, rewards, next_states, dones = map(lambda x: torch.tensor(x).float(), zip(*sample_experience_replay_buffer(batch_size)))\n",
    "    \n",
    "        # compute loss\n",
    "        loss = -torch.mean(network_critic(states, network_actor(states)))\n",
    "    \n",
    "        # train\n",
    "        optimizer_actor.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(network_actor.parameters(), max_norm=1)\n",
    "        optimizer_actor.step()\n",
    "    \n",
    "    while True:\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        for _ in range(1600):\n",
    "            if done: break\n",
    "            action = RandomAgent(m).forward(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "            experience_replay_buffer.append((state, action, reward, next_state, done))\n",
    "            \n",
    "            state = next_state\n",
    "    \n",
    "        if len(experience_replay_buffer) == ERB_size:\n",
    "            break\n",
    "        \n",
    "    noise = np.zeros((1, 2,))\n",
    "    EPISODES = trange(N_episodes, desc='Episode: ', leave=True)\n",
    "    for i in EPISODES:\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        total_episode_reward = 0.\n",
    "        epsilon = epsilon0 * np.exp(- 3 * i / N_episodes)\n",
    "        t = 0\n",
    "        #while not done:\n",
    "            # act\n",
    "        for _ in range(1600):\n",
    "            if done: break\n",
    "            action = network_actor.act(state, noise).ravel()\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            total_episode_reward += reward\n",
    "    \n",
    "            # train\n",
    "            train_network_critic(network_actor, network_critic, target_actor, target_critic)\n",
    "            if t % d == 0:\n",
    "                train_network_actor(network_actor, network_critic)\n",
    "                train_target(network_actor, network_critic, target_actor, target_critic)\n",
    "            \n",
    "            # update\n",
    "            experience_replay_buffer.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            t += 1\n",
    "            noise = -mu * noise + sigma * np.random.randn(1, 2)\n",
    "    \n",
    "        episode_reward_list.append(total_episode_reward)\n",
    "        episode_number_of_steps.append(t)\n",
    "    \n",
    "        env.close()\n",
    "    \n",
    "        EPISODES.set_description(\n",
    "            \"Episode {} - Reward/Steps: {:.1f}/{} - Avg. Reward/Steps: {:.1f}/{}\".format(\n",
    "            i, total_episode_reward, t,\n",
    "            running_average(episode_reward_list, n_ep_running_average)[-1],\n",
    "            running_average(episode_number_of_steps, n_ep_running_average)[-1]))\n",
    "        #if running_average(episode_reward_list, n_ep_running_average)[-1] > 200:\n",
    "        #    break\n",
    "    \n",
    "    while len(episode_reward_list) < N_episodes:\n",
    "        episode_reward_list.append(episode_reward_list[-1])\n",
    "        episode_number_of_steps.append(episode_number_of_steps[-1])\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 9))\n",
    "    ax[0].plot([i for i in range(1, N_episodes+1)], episode_reward_list, label='Episode reward')\n",
    "    ax[0].plot([i for i in range(1, N_episodes+1)], running_average(\n",
    "        episode_reward_list, n_ep_running_average), label='Avg. episode reward')\n",
    "    ax[0].set_xlabel('Episodes')\n",
    "    ax[0].set_ylabel('Total reward')\n",
    "    ax[0].set_title('Total Reward vs Episodes')\n",
    "    ax[0].legend()\n",
    "    ax[0].grid(alpha=0.3)\n",
    "    \n",
    "    ax[1].plot([i for i in range(1, N_episodes+1)], episode_number_of_steps, label='Steps per episode')\n",
    "    ax[1].plot([i for i in range(1, N_episodes+1)], running_average(\n",
    "        episode_number_of_steps, n_ep_running_average), label='Avg. number of steps per episode')\n",
    "    ax[1].set_xlabel('Episodes')\n",
    "    ax[1].set_ylabel('Total number of steps')\n",
    "    ax[1].set_title('Total number of steps vs Episodes')\n",
    "    ax[1].legend()\n",
    "    ax[1].grid(alpha=0.3)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
