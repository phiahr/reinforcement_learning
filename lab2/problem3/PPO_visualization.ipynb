{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from tqdm import trange\n",
    "from torch import nn\n",
    "from PPO_agent import RandomAgent\n",
    "from PPO import Actor, Critic\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "def running_average(x, N):\n",
    "    ''' Function used to compute the running average\n",
    "        of the last N elements of a vector x\n",
    "    '''\n",
    "    if len(x) >= N:\n",
    "        y = np.copy(x)\n",
    "        y[N-1:] = np.convolve(x, np.ones((N, )) / N, mode='valid')\n",
    "    else:\n",
    "        y = np.zeros_like(x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:   0%|          | 0/1600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_169178/1011122353.py:43: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  states, actions, rewards, next_states, dones = map(lambda x: torch.tensor(x).float().reshape((t, -1)), zip(*experience_replay_buffer))\n",
      "Episode 134 - Reward/Steps: -282.1/125 - Avg. Reward/Steps: -233.3/138:   8%|▊         | 135/1600 [10:21<2:17:06,  5.62s/it]/tmp/ipykernel_169178/1011122353.py:48: RuntimeWarning: overflow encountered in power\n",
      "  g = discount_factor**(k - i)\n",
      "Episode 178 - Reward/Steps: -261.7/153 - Avg. Reward/Steps: -289.3/232:  11%|█         | 179/1600 [15:08<2:06:54,  5.36s/it] "
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env.reset()\n",
    "\n",
    "# State/action space description\n",
    "S = 8\n",
    "A = 2\n",
    "E = S + A + 1 + S + 1 # S, A, R, S', done\n",
    "\n",
    "# Utility parameters\n",
    "n_ep_running_average = 50                    \n",
    "m = len(env.action_space.high) # dimensionality of the action\n",
    "dim_state = len(env.observation_space.high)  \n",
    "\n",
    "# Hyperparameters\n",
    "ep = 0.2\n",
    "M = 10\n",
    "N_episodes = 1600\n",
    "discount_factor = 0.99\n",
    "critic_learning_rate = 1e-3\n",
    "actor_learning_rate = 1e-5\n",
    "\n",
    "for discount_factor in [ 0.2, 0.99, 1 ]:\n",
    "    episode_reward_list = [] \n",
    "    episode_number_of_steps = [] \n",
    "    # Experience replay buffer\n",
    "    experience_replay_buffer = []\n",
    "    \n",
    "    # Initialize networks, optimizer and loss function\n",
    "    network_actor = Actor(S, A)\n",
    "    network_critic = Critic(S, A)\n",
    "    \n",
    "    optimizer_critic = torch.optim.Adam(network_critic.parameters(), lr=critic_learning_rate)\n",
    "    optimizer_actor = torch.optim.Adam(network_actor.parameters(), lr=actor_learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    def clip(x):\n",
    "        _ep = torch.tensor(len(x) * [ ep ]).reshape((-1, 1))\n",
    "        return torch.max(1 - _ep, torch.min(1 + _ep, x))\n",
    "    \n",
    "    def train_network(network_actor: Actor, network_critic: Critic):\n",
    "        # create torch tensors from experience replay buffer\n",
    "        t = len(experience_replay_buffer)\n",
    "        states, actions, rewards, next_states, dones = map(lambda x: torch.tensor(x).float().reshape((t, -1)), zip(*experience_replay_buffer))\n",
    "    \n",
    "        # compute G\n",
    "        i = np.arange(t).reshape((-1, 1))\n",
    "        k = np.arange(t).reshape((1, -1))\n",
    "        g = discount_factor**(k - i)\n",
    "        g[~(k >= i)] = 0\n",
    "        G = torch.tensor(g).float() @ rewards\n",
    "    \n",
    "        # compute advantage estimation\n",
    "        advantage_estimation = (G - network_critic(states)).detach()\n",
    "    \n",
    "        # compute old log probabilities\n",
    "        mu, cov = network_actor(states)\n",
    "        mu_old = mu.detach()\n",
    "        cov_old = cov.detach()\n",
    "        mv_old = MultivariateNormal(mu_old, torch.diag_embed(cov_old))\n",
    "        log_probs_old = mv_old.log_prob(actions)\n",
    "    \n",
    "        # update networks\n",
    "        for _ in range(M):\n",
    "            \"\"\" critic update \"\"\"\n",
    "            optimizer_critic.zero_grad()\n",
    "    \n",
    "            # compute loss\n",
    "            loss_critic = loss_fn(G, network_critic(states))\n",
    "    \n",
    "            loss_critic.backward()\n",
    "            nn.utils.clip_grad_norm_(network_critic.parameters(), max_norm=1)\n",
    "            optimizer_critic.step()\n",
    "    \n",
    "            \"\"\" actor update \"\"\"\n",
    "            optimizer_actor.zero_grad()\n",
    "    \n",
    "            # compute new log probabilities\n",
    "            mu, cov = network_actor(states)\n",
    "            mv = MultivariateNormal(mu, torch.diag_embed(cov))\n",
    "            log_probs = mv.log_prob(actions)\n",
    "    \n",
    "            # exp(log(a) - log(b)) = a / b\n",
    "            r_theta = torch.exp(log_probs - log_probs_old).reshape((-1, 1))\n",
    "    \n",
    "            # compute loss\n",
    "            loss_actor = -torch.mean(torch.minimum(\n",
    "                r_theta * advantage_estimation,\n",
    "                clip(r_theta) * advantage_estimation\n",
    "            ))\n",
    "    \n",
    "            loss_actor.backward()\n",
    "            nn.utils.clip_grad_norm_(network_actor.parameters(), max_norm=1)\n",
    "            optimizer_actor.step()\n",
    "    \n",
    "    EPISODES = trange(N_episodes, desc='Episode: ', leave=True)\n",
    "    for i in EPISODES:\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        total_episode_reward = 0.\n",
    "        experience_replay_buffer = []\n",
    "        t = 0\n",
    "        #while not done:\n",
    "        for _ in range(3200):\n",
    "            if done: break\n",
    "            action = network_actor.act(state).ravel()\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            total_episode_reward += reward\n",
    "    \n",
    "            # train\n",
    "            experience_replay_buffer.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            t += 1\n",
    "        \n",
    "        train_network(network_actor, network_critic)\n",
    "    \n",
    "        episode_reward_list.append(total_episode_reward)\n",
    "        episode_number_of_steps.append(t)\n",
    "    \n",
    "        env.close()\n",
    "    \n",
    "        EPISODES.set_description(\n",
    "            \"Episode {} - Reward/Steps: {:.1f}/{} - Avg. Reward/Steps: {:.1f}/{}\".format(\n",
    "            i, total_episode_reward, t,\n",
    "            running_average(episode_reward_list, n_ep_running_average)[-1],\n",
    "            running_average(episode_number_of_steps, n_ep_running_average)[-1]))\n",
    "    \n",
    "    while len(episode_reward_list) < N_episodes:\n",
    "        episode_reward_list.append(episode_reward_list[-1])\n",
    "        episode_number_of_steps.append(episode_number_of_steps[-1])\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 9))\n",
    "    ax[0].plot([i for i in range(1, N_episodes+1)], episode_reward_list, label='Episode reward')\n",
    "    ax[0].plot([i for i in range(1, N_episodes+1)], running_average(\n",
    "        episode_reward_list, n_ep_running_average), label='Avg. episode reward')\n",
    "    ax[0].set_xlabel('Episodes')\n",
    "    ax[0].set_ylabel('Total reward')\n",
    "    ax[0].set_title('Total Reward vs Episodes')\n",
    "    ax[0].legend()\n",
    "    ax[0].grid(alpha=0.3)\n",
    "    \n",
    "    ax[1].plot([i for i in range(1, N_episodes+1)], episode_number_of_steps, label='Steps per episode')\n",
    "    ax[1].plot([i for i in range(1, N_episodes+1)], running_average(\n",
    "        episode_number_of_steps, n_ep_running_average), label='Avg. number of steps per episode')\n",
    "    ax[1].set_xlabel('Episodes')\n",
    "    ax[1].set_ylabel('Total number of steps')\n",
    "    ax[1].set_title('Total number of steps vs Episodes')\n",
    "    ax[1].legend()\n",
    "    ax[1].grid(alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env.reset()\n",
    "\n",
    "# State/action space description\n",
    "S = 8\n",
    "A = 2\n",
    "E = S + A + 1 + S + 1 # S, A, R, S', done\n",
    "\n",
    "# Utility parameters\n",
    "n_ep_running_average = 50                    \n",
    "m = len(env.action_space.high) # dimensionality of the action\n",
    "dim_state = len(env.observation_space.high)  \n",
    "\n",
    "# Hyperparameters\n",
    "ep = 0.2\n",
    "M = 10\n",
    "N_episodes = 1600\n",
    "discount_factor = 0.99\n",
    "critic_learning_rate = 1e-3\n",
    "actor_learning_rate = 1e-5\n",
    "\n",
    "for ep in [ 0.1, 0.2, 0.5 ]:\n",
    "    episode_reward_list = [] \n",
    "    episode_number_of_steps = [] \n",
    "    # Experience replay buffer\n",
    "    experience_replay_buffer = []\n",
    "    \n",
    "    # Initialize networks, optimizer and loss function\n",
    "    network_actor = Actor(S, A)\n",
    "    network_critic = Critic(S, A)\n",
    "    \n",
    "    optimizer_critic = torch.optim.Adam(network_critic.parameters(), lr=critic_learning_rate)\n",
    "    optimizer_actor = torch.optim.Adam(network_actor.parameters(), lr=actor_learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    def clip(x):\n",
    "        _ep = torch.tensor(len(x) * [ ep ]).reshape((-1, 1))\n",
    "        return torch.max(1 - _ep, torch.min(1 + _ep, x))\n",
    "    \n",
    "    def train_network(network_actor: Actor, network_critic: Critic):\n",
    "        # create torch tensors from experience replay buffer\n",
    "        t = len(experience_replay_buffer)\n",
    "        states, actions, rewards, next_states, dones = map(lambda x: torch.tensor(x).float().reshape((t, -1)), zip(*experience_replay_buffer))\n",
    "    \n",
    "        # compute G\n",
    "        i = np.arange(t).reshape((-1, 1))\n",
    "        k = np.arange(t).reshape((1, -1))\n",
    "        g = discount_factor**(k - i)\n",
    "        g[~(k >= i)] = 0\n",
    "        G = torch.tensor(g).float() @ rewards\n",
    "    \n",
    "        # compute advantage estimation\n",
    "        advantage_estimation = (G - network_critic(states)).detach()\n",
    "    \n",
    "        # compute old log probabilities\n",
    "        mu, cov = network_actor(states)\n",
    "        mu_old = mu.detach()\n",
    "        cov_old = cov.detach()\n",
    "        mv_old = MultivariateNormal(mu_old, torch.diag_embed(cov_old))\n",
    "        log_probs_old = mv_old.log_prob(actions)\n",
    "    \n",
    "        # update networks\n",
    "        for _ in range(M):\n",
    "            \"\"\" critic update \"\"\"\n",
    "            optimizer_critic.zero_grad()\n",
    "    \n",
    "            # compute loss\n",
    "            loss_critic = loss_fn(G, network_critic(states))\n",
    "    \n",
    "            loss_critic.backward()\n",
    "            nn.utils.clip_grad_norm_(network_critic.parameters(), max_norm=1)\n",
    "            optimizer_critic.step()\n",
    "    \n",
    "            \"\"\" actor update \"\"\"\n",
    "            optimizer_actor.zero_grad()\n",
    "    \n",
    "            # compute new log probabilities\n",
    "            mu, cov = network_actor(states)\n",
    "            mv = MultivariateNormal(mu, torch.diag_embed(cov))\n",
    "            log_probs = mv.log_prob(actions)\n",
    "    \n",
    "            # exp(log(a) - log(b)) = a / b\n",
    "            r_theta = torch.exp(log_probs - log_probs_old).reshape((-1, 1))\n",
    "    \n",
    "            # compute loss\n",
    "            loss_actor = -torch.mean(torch.minimum(\n",
    "                r_theta * advantage_estimation,\n",
    "                clip(r_theta) * advantage_estimation\n",
    "            ))\n",
    "    \n",
    "            loss_actor.backward()\n",
    "            nn.utils.clip_grad_norm_(network_actor.parameters(), max_norm=1)\n",
    "            optimizer_actor.step()\n",
    "    \n",
    "    EPISODES = trange(N_episodes, desc='Episode: ', leave=True)\n",
    "    for i in EPISODES:\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        total_episode_reward = 0.\n",
    "        experience_replay_buffer = []\n",
    "        t = 0\n",
    "        #while not done:\n",
    "        for _ in range(3200):\n",
    "            if done: break\n",
    "            action = network_actor.act(state).ravel()\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            total_episode_reward += reward\n",
    "    \n",
    "            # train\n",
    "            experience_replay_buffer.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            t += 1\n",
    "        \n",
    "        train_network(network_actor, network_critic)\n",
    "    \n",
    "        episode_reward_list.append(total_episode_reward)\n",
    "        episode_number_of_steps.append(t)\n",
    "    \n",
    "        env.close()\n",
    "    \n",
    "        EPISODES.set_description(\n",
    "            \"Episode {} - Reward/Steps: {:.1f}/{} - Avg. Reward/Steps: {:.1f}/{}\".format(\n",
    "            i, total_episode_reward, t,\n",
    "            running_average(episode_reward_list, n_ep_running_average)[-1],\n",
    "            running_average(episode_number_of_steps, n_ep_running_average)[-1]))\n",
    "    \n",
    "    while len(episode_reward_list) < N_episodes:\n",
    "        episode_reward_list.append(episode_reward_list[-1])\n",
    "        episode_number_of_steps.append(episode_number_of_steps[-1])\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 9))\n",
    "    ax[0].plot([i for i in range(1, N_episodes+1)], episode_reward_list, label='Episode reward')\n",
    "    ax[0].plot([i for i in range(1, N_episodes+1)], running_average(\n",
    "        episode_reward_list, n_ep_running_average), label='Avg. episode reward')\n",
    "    ax[0].set_xlabel('Episodes')\n",
    "    ax[0].set_ylabel('Total reward')\n",
    "    ax[0].set_title('Total Reward vs Episodes')\n",
    "    ax[0].legend()\n",
    "    ax[0].grid(alpha=0.3)\n",
    "    \n",
    "    ax[1].plot([i for i in range(1, N_episodes+1)], episode_number_of_steps, label='Steps per episode')\n",
    "    ax[1].plot([i for i in range(1, N_episodes+1)], running_average(\n",
    "        episode_number_of_steps, n_ep_running_average), label='Avg. number of steps per episode')\n",
    "    ax[1].set_xlabel('Episodes')\n",
    "    ax[1].set_ylabel('Total number of steps')\n",
    "    ax[1].set_title('Total number of steps vs Episodes')\n",
    "    ax[1].legend()\n",
    "    ax[1].grid(alpha=0.3)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
